{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14105446,"sourceType":"datasetVersion","datasetId":8984036,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"oumniyya/data-competition\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T19:31:51.706683Z","iopub.execute_input":"2025-12-11T19:31:51.707394Z","iopub.status.idle":"2025-12-11T19:31:53.155306Z","shell.execute_reply.started":"2025-12-11T19:31:51.707369Z","shell.execute_reply":"2025-12-11T19:31:53.154459Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/data-competition\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sklearn.decomposition import PCA\nimport torch.nn as nn\n\n# =================CONFIG=================\n\nIMG_DIR = \"/kaggle/input/data-competition/data/item_images\"\nITEM_INFO_PATH = \"/kaggle/input/data-competition/data/MicroLens_1M_x1/item_info.parquet\"\nOUTPUT_PATH = \"item_info_new_fusion.parquet\" # fichier pour la soumission\n\nBATCH_SIZE = 128\nNUM_WORKERS = 0 # Mis √† 0 pour √©viter les erreurs Assertion Error\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTARGET_DIM = 128  # OBLIGATOIRE pour le mod√®le DIN\n# ========================================\\\n\nprint(f\"Using Device: {DEVICE}\")\n\n# 1. Chargement des donn√©es\nprint(\"Chargement de item_info.parquet...\")\ndf_items = pd.read_parquet(ITEM_INFO_PATH)\nprint(df_items.head(2))\n\n# 2. Dataset Custom pour PyTorch\nclass MultimodalDataset(Dataset):\n    def __init__(self, df, img_dir, processor):\n        self.df = df\n        self.img_dir = img_dir\n        self.processor = processor\n        self.item_ids = df['item_id'].tolist()\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        item_id = self.item_ids[idx]\n        \n        # Gestion du chemin image\n        img_path = os.path.join(self.img_dir, f\"{item_id}.jpg\") \n        \n        # Gestion Texte (Titre ou Description)\n        text = \"Item\" # Fallback si pas de texte\n        if 'title' in self.df.columns:\n            text = str(self.df.iloc[idx]['title'])\n            \n        # Chargement Image (avec gestion d'erreur si image manquante)\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except:\n            # Si image vide/manquante, on cr√©e une image noire\n            image = Image.new('RGB', (224, 224), color='black')\n\n        # Processing CLIP\n        inputs = self.processor(text=[text], images=image, return_tensors=\"pt\", padding=True, truncation=True)\n        \n        # On retire la dimension de batch ajout√©e par le processor\n        return {k: v.squeeze(0) for k, v in inputs.items()}\n\n# 3. Initialisation Mod√®le (CLIP)\nprint(\"Chargement du mod√®le CLIP...\")\nmodel_id = \"openai/clip-vit-base-patch32\"\nprocessor = CLIPProcessor.from_pretrained(model_id)\nmodel = CLIPModel.from_pretrained(model_id)\n\nif torch.cuda.device_count() > 1:\n    model = nn.DataParallel(model)\n\nmodel.to(DEVICE)\nmodel.eval()\n\n# 4. Extraction des Embeddings\ndataset = MultimodalDataset(df_items, IMG_DIR, processor)\n#  num_workers=0 pour la stabilit√©\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n\nall_embeddings = []\n\nprint(\"D√©but de l'extraction des features (Fusion Concat√©nation)...\")\nwith torch.no_grad():\n    for batch in tqdm(dataloader):\n        # Envoi sur GPU\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n        \n        # Passage dans le mod√®le\n        outputs = model(**batch)\n        \n        img_emb = outputs.image_embeds # [Batch, 512]\n        txt_emb = outputs.text_embeds  # [Batch, 512]\n        \n        #  Concat√©nation (512 + 512 = 1024)\n        fused_emb = torch.cat([img_emb, txt_emb], dim=-1) # [Batch, 1024]\n        \n        all_embeddings.append(fused_emb.cpu().numpy())\n\n# Concat√©ner tous les batchs -> Matrice (N_items, 1024)\nfull_embeddings = np.concatenate(all_embeddings, axis=0)\nprint(f\"Features extraites. Shape: {full_embeddings.shape} (1024D)\")\n\n# 5. R√©duction de Dimension (1024 -> 128) via PCA\nprint(\"R√©duction de dimension vers 128 (PCA)...\")\n#  PCA est maintenant de 1024 -> 128\npca = PCA(n_components=TARGET_DIM)\nreduced_embeddings = pca.fit_transform(full_embeddings)\n\n# Normalisation finale pour aider le mod√®le DIN\nnorms = np.linalg.norm(reduced_embeddings, axis=1, keepdims=True)\nreduced_embeddings = reduced_embeddings / (norms + 1e-8)\n\nprint(f\"Shape finale : {reduced_embeddings.shape}\")\n\n# 6. Sauvegarde \nprint(\"Mise √† jour du DataFrame...\")\n\n# 6a. Supprimer la/les ancienne(s) colonne(s) d'embedding si elle(s) existe(nt)\n# Nous v√©rifions √† la fois l'ancien nom interm√©diaire ('item_emb') et le nom final ('item_emb_d128')\nfor col in ['item_emb', 'item_emb_d128']:\n    if col in df_items.columns:\n        print(f\"üîß Suppression de l'ancienne colonne: {col}\")\n        df_items = df_items.drop(columns=[col])\n\n# 6b. Cr√©er la nouvelle colonne avec le nom final\ndf_items['item_emb_d128'] = list(reduced_embeddings)\n\nprint(f\"Sauvegarde dans {OUTPUT_PATH}...\")\ndf_items.to_parquet(OUTPUT_PATH)\n\nprint(\"Termin√©! Le nouveau fichier d'embeddings est pr√™t pour la Task 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T20:09:30.671145Z","iopub.execute_input":"2025-12-11T20:09:30.671921Z","iopub.status.idle":"2025-12-11T20:27:31.411895Z","shell.execute_reply.started":"2025-12-11T20:09:30.671896Z","shell.execute_reply":"2025-12-11T20:27:31.411198Z"}},"outputs":[{"name":"stdout","text":"Using Device: cuda\nChargement de item_info.parquet...\n   item_id        item_tags                                      item_emb_d128\n0        0  [0, 0, 0, 0, 0]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n1        1  [0, 0, 0, 0, 1]  [-0.587724506855011, -0.38462838530540466, 0.4...\nChargement du mod√®le CLIP...\nD√©but de l'extraction des features (Fusion Concat√©nation)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/717 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bb2e38823fd439782e2d35b7ee943a3"}},"metadata":{}},{"name":"stdout","text":"Features extraites. Shape: (91718, 1024) (1024D)\nR√©duction de dimension vers 128 (PCA)...\nShape finale : (91718, 128)\nMise √† jour du DataFrame...\nüîß Suppression de l'ancienne colonne: item_emb_d128\nSauvegarde dans item_info_new_fusion.parquet...\nTermin√©! Le nouveau fichier d'embeddings est pr√™t pour la Task 2.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ==================== CONFIGURATION (MOD√àLE CTR GAGNANT) ====================\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"üîß Device: {DEVICE}\")\n\n# Chemins des donn√©es\n\nITEM_INFO_PATH = \"/kaggle/working/item_info_new_fusion.parquet\"\nTRAIN_PATH = \"/kaggle/input/data-competition/data/MicroLens_1M_x1/train.parquet\"\nVALID_PATH = \"/kaggle/input/data-competition/data/MicroLens_1M_x1/valid.parquet\"\nTEST_PATH = \"/kaggle/input/data-competition/data/MicroLens_1M_x1/test.parquet\"\nITEM_SEQ_PATH = \"/kaggle/input/data-competition/data/item_seq.parquet\"\n\n# Hyperparam√®tres \nBATCH_SIZE = 4096\nEPOCHS = 15\nLR = 0.002\nWEIGHT_DECAY = 5e-6 # Petite r√©gularisation pour la stabilit√©\nDROPOUT = 0.3\nPATIENCE = 4\n\n\nEMB_DIM = 64\nHIDDEN_DIMS = [512, 256, 128]\nATTENTION_DIM = 256\n\n# ==================== DATASET ====================\nclass CTRDataset(Dataset):\n    def __init__(self, df, item_info, item_seq, max_seq_len=50):\n        self.df = df.reset_index(drop=True)\n        self.item_info = item_info\n        self.item_seq = item_seq\n        self.max_seq_len = max_seq_len\n        \n        # Merge item features\n        self.df = pd.merge(self.df, item_info, on='item_id', how='left')\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Features de base\n        user_id = row['user_id']\n        item_id = row['item_id']\n        likes_level = row.get('likes_level', 0)\n        views_level = row.get('views_level', 0)\n        \n        # Embedding multimodal (Task 1)\n        item_emb = np.array(row['item_emb_d128'])\n        \n        # Tags de l'item (gestion robuste)\n        item_tags_val = row.get('item_tags', [])\n        if isinstance(item_tags_val, np.ndarray):\n            item_tags_val = item_tags_val.tolist()\n        if not isinstance(item_tags_val, list):\n            item_tags_val = []\n            \n        # Filtrer les valeurs non valides et tronquer/padder\n        item_tags = [int(x) for x in item_tags_val if x is not None and x > 0]\n        item_tags = item_tags[:5] + [0] * (5 - len(item_tags))\n        \n        # S√©quence historique\n        hist_seq = self.item_seq.get(user_id, [])\n        \n        # Conversion en liste si c'est un numpy array\n        if isinstance(hist_seq, np.ndarray):\n            hist_seq = hist_seq.tolist()\n        elif not isinstance(hist_seq, list):\n            hist_seq = []\n        \n        if len(hist_seq) > self.max_seq_len:\n            hist_seq = hist_seq[-self.max_seq_len:]\n        \n        hist_len = len(hist_seq)\n        hist_seq = hist_seq + [0] * (self.max_seq_len - len(hist_seq))\n        \n        # Label\n        label = row.get('label', 0)\n        \n        return {\n            'user_id': user_id,\n            'item_id': item_id,\n            'item_emb': torch.FloatTensor(item_emb),\n            'item_tags': torch.LongTensor(item_tags),\n            'likes_level': likes_level,\n            'views_level': views_level,\n            'hist_seq': torch.LongTensor(hist_seq),\n            'hist_len': torch.LongTensor([hist_len]),\n            'label': torch.FloatTensor([label])\n        }\n\n# ==================== MOD√àLE CTR (Attention/DNN) ====================\nclass MultimodalAttentionCTR(nn.Module):\n    def __init__(self, n_users, n_items, n_tags, emb_dim=64, \n                 mm_dim=128, hidden_dims=HIDDEN_DIMS, dropout=DROPOUT):\n        super().__init__()\n        \n        self.user_emb = nn.Embedding(n_users + 1, emb_dim, padding_idx=0)\n        self.item_emb = nn.Embedding(n_items + 1, emb_dim, padding_idx=0)\n        self.tag_emb = nn.Embedding(n_tags + 1, emb_dim, padding_idx=0)\n        \n        self.mm_proj = nn.Sequential(\n            nn.Linear(mm_dim, emb_dim),\n            nn.LayerNorm(emb_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout * 0.5)\n        )\n        \n        self.attention = nn.MultiheadAttention(\n            embed_dim=emb_dim,\n            num_heads=4,\n            dropout=dropout * 0.5,\n            batch_first=True\n        )\n        \n        input_dim = emb_dim * 5 + 2 \n        \n        layers = []\n        prev_dim = input_dim\n        for hdim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hdim),\n                nn.BatchNorm1d(hdim),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n            ])\n            prev_dim = hdim\n        \n        layers.append(nn.Linear(prev_dim, 1))\n        self.dnn = nn.Sequential(*layers)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Embedding):\n                nn.init.normal_(m.weight, std=0.01)\n    \n    def forward(self, user_id, item_id, item_emb, item_tags, \n                likes_level, views_level, hist_seq, hist_len):\n        \n        user_vec = self.user_emb(user_id)\n        item_vec = self.item_emb(item_id)\n        mm_vec = self.mm_proj(item_emb)\n        tags_vec = self.tag_emb(item_tags).mean(dim=1)\n        \n        hist_emb = self.item_emb(hist_seq)\n        \n        # Masque pour l'attention\n        mask = torch.arange(hist_seq.size(1), device=hist_seq.device)[None, :] >= hist_len[:, None]\n        \n        # Self-attention sur l'historique\n        hist_att, _ = self.attention(\n            hist_emb, hist_emb, hist_emb,\n            key_padding_mask=mask\n        )\n        \n        hist_vec = hist_att.sum(dim=1) / (hist_len.unsqueeze(1) + 1e-8)\n        \n        likes_vec = likes_level.float().unsqueeze(1) / 10.0\n        views_vec = views_level.float().unsqueeze(1) / 10.0\n        \n        combined = torch.cat([\n            user_vec, item_vec, mm_vec, tags_vec, hist_vec,\n            likes_vec, views_vec\n        ], dim=1)\n        \n        logits = self.dnn(combined)\n        return torch.sigmoid(logits)\n\n\n# ==================== CHARGEMENT DES DONN√âES ====================\nprint(\"üìÇ Chargement des donn√©es...\")\n\ndf_train = pd.read_parquet(TRAIN_PATH)\ndf_valid = pd.read_parquet(VALID_PATH)\ndf_test = pd.read_parquet(TEST_PATH)\ndf_item_info = pd.read_parquet(ITEM_INFO_PATH)\n\n# V√©rification de la colonne d'embeddings\nif 'item_emb_d128' not in df_item_info.columns:\n    if 'item_emb' in df_item_info.columns:\n        df_item_info['item_emb_d128'] = df_item_info['item_emb']\n    else:\n        raise ValueError(\"‚ùå Colonne 'item_emb_d128' introuvable!\")\n\n# Chargement des s√©quences\ndf_seq = pd.read_parquet(ITEM_SEQ_PATH)\nitem_seq_dict = dict(zip(df_seq['user_id'], df_seq['item_seq']))\n\n# Statistiques\nn_users = max(df_train['user_id'].max(), df_valid['user_id'].max()) + 1\nn_items = df_item_info['item_id'].max() + 1\nn_tags = 11740 \n\nprint(f\"‚úÖ Users: {n_users:,} | Items: {n_items:,} | Train: {len(df_train):,}\")\n\n# Datasets\ntrain_dataset = CTRDataset(df_train, df_item_info, item_seq_dict)\nvalid_dataset = CTRDataset(df_valid, df_item_info, item_seq_dict)\n\n# num_workers=0 pour la stabilit√©\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n# ==================== ENTRA√éNEMENT ====================\nmodel = MultimodalAttentionCTR(\n    n_users=n_users,\n    n_items=n_items,\n    n_tags=n_tags,\n    emb_dim=EMB_DIM,\n    mm_dim=128,\n    hidden_dims=HIDDEN_DIMS,\n    dropout=DROPOUT\n).to(DEVICE)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n)\n\nprint(f\"\\nüöÄ D√©but de l'entra√Ænement (Task 2 - VRAI FINETUNING) | Epochs: {EPOCHS} | Batch Size: {BATCH_SIZE}\")\nprint(\"=\" * 70)\n\nbest_auc = 0.0\npatience_counter = 0\n\nfor epoch in range(EPOCHS):\n    # Train\n    model.train()\n    train_loss = 0.0\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        optimizer.zero_grad()\n        \n        hist_len = batch['hist_len'].to(DEVICE)\n        \n        preds = model(\n            batch['user_id'].to(DEVICE),\n            batch['item_id'].to(DEVICE),\n            batch['item_emb'].to(DEVICE),\n            batch['item_tags'].to(DEVICE),\n            batch['likes_level'].to(DEVICE),\n            batch['views_level'].to(DEVICE),\n            batch['hist_seq'].to(DEVICE),\n            hist_len.squeeze(1)\n        )\n        \n        loss = criterion(preds, batch['label'].to(DEVICE))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    \n    # Validation\n    model.eval()\n    val_preds, val_labels = [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(valid_loader, desc=\"Validation\"):\n            hist_len = batch['hist_len'].to(DEVICE)\n            \n            preds = model(\n                batch['user_id'].to(DEVICE),\n                batch['item_id'].to(DEVICE),\n                batch['item_emb'].to(DEVICE),\n                batch['item_tags'].to(DEVICE),\n                batch['likes_level'].to(DEVICE),\n                batch['views_level'].to(DEVICE),\n                batch['hist_seq'].to(DEVICE),\n                hist_len.squeeze(1)\n            )\n            \n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(batch['label'].numpy())\n    \n    auc = roc_auc_score(val_labels, val_preds)\n    scheduler.step(auc)\n    \n    print(f\"üìä Epoch {epoch+1} | Loss: {avg_train_loss:.4f} | AUC: {auc:.4f}\")\n    \n    # Early stopping\n    if auc > best_auc:\n        best_auc = auc\n        patience_counter = 0\n        torch.save(model.state_dict(), '/kaggle/working/best_model_new_emb.pt')\n        print(f\"‚úÖ Nouveau meilleur mod√®le sauvegard√©! AUC: {best_auc:.4f}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= PATIENCE:\n            print(f\"‚èπÔ∏è Early stopping apr√®s {epoch+1} epochs\")\n            break\n\nprint(f\"\\nüéâ Entra√Ænement termin√©! Meilleur AUC: {best_auc:.4f}\")\n\n# ==================== PR√âDICTION ====================\nprint(\"\\nüîÆ G√©n√©ration des pr√©dictions...\")\n\n# Charger le mod√®le CTR sauvegard√©\nmodel.load_state_dict(torch.load('/kaggle/working/best_model_new_emb.pt'))\nmodel.eval()\n\ntest_dataset = CTRDataset(df_test, df_item_info, item_seq_dict)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\ntest_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Test\"):\n        hist_len = batch['hist_len'].to(DEVICE)\n\n        preds = model(\n            batch['user_id'].to(DEVICE),\n            batch['item_id'].to(DEVICE),\n            batch['item_emb'].to(DEVICE),\n            batch['item_tags'].to(DEVICE),\n            batch['likes_level'].to(DEVICE),\n            batch['views_level'].to(DEVICE),\n            batch['hist_seq'].to(DEVICE),\n            hist_len.squeeze(1)\n        )\n        test_preds.extend(preds.cpu().numpy().flatten())\n\n# Sauvegarde\nsubmission = pd.DataFrame({\n    'ID': range(len(test_preds)),\n    'Task1': 0,\n    'Task2': 0,\n    'Task1&2': test_preds\n})\n\nsubmission.to_csv('/kaggle/working/submission_task1_2_new_emb.csv', index=False)\nprint(f\"‚úÖ Soumission sauvegard√©e: submission_task1_2_new_emb.csv\")\nprint(f\"üìà Meilleur AUC validation: {best_auc:.4f}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T20:35:07.616589Z","iopub.execute_input":"2025-12-11T20:35:07.616918Z","iopub.status.idle":"2025-12-11T21:28:57.449401Z","shell.execute_reply.started":"2025-12-11T20:35:07.616894Z","shell.execute_reply":"2025-12-11T21:28:57.448413Z"}},"outputs":[{"name":"stdout","text":"üîß Device: cuda\nüìÇ Chargement des donn√©es...\n‚úÖ Users: 1,000,001 | Items: 91,718 | Train: 3,600,000\n\nüöÄ D√©but de l'entra√Ænement (Task 2 - VRAI FINETUNING) | Epochs: 15 | Batch Size: 4096\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/15:   0%|          | 0/879 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b48be02b07ee42eba20394b96d37289c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1faa1dad2a3a45bd94609feae2e81c79"}},"metadata":{}},{"name":"stdout","text":"üìä Epoch 1 | Loss: 0.2055 | AUC: 0.7167\n‚úÖ Nouveau meilleur mod√®le sauvegard√©! AUC: 0.7167\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/15:   0%|          | 0/879 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18ab0909f571452792a998ba72577c30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b77cfa62e61f40ebb73ff41dda342f9a"}},"metadata":{}},{"name":"stdout","text":"üìä Epoch 2 | Loss: 0.0653 | AUC: 0.7752\n‚úÖ Nouveau meilleur mod√®le sauvegard√©! AUC: 0.7752\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/15:   0%|          | 0/879 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b71be3f5da4006b37b2139619dacf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ad199122fe457a8d7a919b4cc1fd11"}},"metadata":{}},{"name":"stdout","text":"üìä Epoch 3 | Loss: 0.0186 | AUC: 0.7530\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/15:   0%|          | 0/879 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f16f83c0fe24b448cacb23189a168b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b743dc2099a6476991d29ea7cbefe59a"}},"metadata":{}},{"name":"stdout","text":"üìä Epoch 4 | Loss: 0.0071 | AUC: 0.7271\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/15:   0%|          | 0/879 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a657694b059404f919106f15fabbeae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"707f133d80724fcd805d24f629e087a8"}},"metadata":{}},{"name":"stdout","text":"üìä Epoch 5 | Loss: 0.0040 | AUC: 0.7376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/15:   0%|          | 0/879 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd155515324f4809aa8976aef4509bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b15174e64cb41cfa3985efa393c7343"}},"metadata":{}},{"name":"stdout","text":"üìä Epoch 6 | Loss: 0.0016 | AUC: 0.7470\n‚èπÔ∏è Early stopping apr√®s 6 epochs\n\nüéâ Entra√Ænement termin√©! Meilleur AUC: 0.7752\n\nüîÆ G√©n√©ration des pr√©dictions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Test:   0%|          | 0/93 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aa5c4e22bb04698ad6164c10de91280"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Soumission sauvegard√©e: submission_task1_2_new_emb.csv\nüìà Meilleur AUC validation: 0.7752\n   ID  Task1  Task2   Task1&2\n0   0      0      0  0.993067\n1   1      0      0  0.999798\n2   2      0      0  0.901648\n3   3      0      0  0.005161\n4   4      0      0  0.002700\n","output_type":"stream"}],"execution_count":4}]}