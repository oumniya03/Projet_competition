#  PrÃ©diction CTR Multimodale sur MicroLens-1M 
## ğŸ“Œ AperÃ§u du Projet

Ce dÃ©pÃ´t contient la solution pour la **CompÃ©tition de PrÃ©diction du Taux de Clics Multimodaux (CTR)** basÃ©e sur le jeu de donnÃ©es **MicroLens-1M**.

L'objectif de ce projet est de prÃ©dire la probabilitÃ© qu'un utilisateur clique sur un Ã©lÃ©ment spÃ©cifique (vidÃ©o/article) en se basant sur :
- **Historique Utilisateur :**  Comportement sÃ©quentiel (clics passÃ©s).
- **Contenu de l'Ã‰lÃ©ment :** CaractÃ©ristiques multimodales (Titres Textuels + Couvertures Images).

Mon approche repose sur une **architecture d'Optimisation en Cascade.** Contrairement Ã  une mÃ©thode 'end-to-end' souvent trop lourde, jâ€™ai sÃ©parÃ© l'extraction de connaissances (Task 1) de la modÃ©lisation comportementale (Task 2). Cette stratÃ©gie mâ€™a permis d'utiliser des modÃ¨les de pointe comme CLIP tout en gardant un modÃ¨le de prÃ©diction agile.

- **Task 1** : Extraction d'embeddings multimodaux avec CLIP
- **Task 2** : ModÃ¨le CTR avec architecture Attention + DNN
- **Meilleur AUC** : 0.7752 (validation)

## ğŸ—ï¸ Architecture
1. **CLIP fine-tuning** : Fusion texte (titre) + image (couverture)
2. **RÃ©duction PCA** : 1024D â†’ 128D
3. **ModÃ¨le CTR** : User/Item embeddings + Multi-head Attention + Deep NN
<img width="1261" height="1364" alt="diagram-export-20-12-2025-12_45_52" src="https://github.com/user-attachments/assets/ca8ea7d4-4550-460c-906a-15632be4201f" />

## ğŸš€ Utilisation
```bash
# ExÃ©cuter le notebook complet
jupyter notebook competition.ipynb
```

## ğŸ“Š RÃ©sultats
- AUC validation : **0.7752**
- Dataset : 3.6M interactions train, 91K items

## ğŸ› ï¸ Stack Technique
PyTorch â€¢ Transformers (CLIP) â€¢ Pandas â€¢ scikit-learn

## ğŸ‘¤ Author

**Oumniya Moutaouakil**
-Master's Student in Advanced Machine Learning & Multimedia Intelligence.
- GitHub: [@oumniya03](https://github.com/oumniya03)
- Project: [Projet-rag](https://github.com/oumniya03/Projet-rag)





